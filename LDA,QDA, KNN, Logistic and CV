---
title: "STATS101C_Hw3"
author: "Seulchan Kim"
date: '2019 10 14 '
output: html_document
---


4.7.4
When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.

(a) Suppose that we have a set of observations, each with measure- ments on p = 1 feature, X. We assume that X is uniformly (evenly) distributed on [0,1]. Associated with each observation is a response value. Suppose that we wish to predict a test obser- vation’s response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0.6, we will use observations in the range [0.55,0.65]. On average, what fraction of the available observations will we use to make the prediction?

$$\int_{0.05}^{0.95}10dx + \int_{0}^{0.05}(100x+5)dx + \int_{0.95}^{1}(105-100x)dx = 9 + 0.375 + 0.375 = 9.75$$

(b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X1 and X2. We assume that (X1,X2) are uniformly distributed on [0,1]×[0,1]. We wish to predict a test observation’s response using only observations that are within 10 % of the range of X1 and within 10 % of the range of X2 closest to that test observation. For instance, in order to predict the response for a test observation with X1 = 0.6 and X2 = 0.35, we will use observations in the range [0.55, 0.65] for X1 and in the range [0.3, 0.4] for X2. On average, what fraction of the available observations will we use to make the prediction?
$$9.75\% \times \ 9.75\% =0.95\% $$

(c) Now suppose that we have a set of observations on p = 100 fea- tures. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?
$$0.1^{100} = \frac{1}{10^{100}}=\frac{1}{10^{98}}\% $$

(d) Using your answers to parts (a)–(c), argue that a drawback of KNN when p is large is that there are very few training obser- vations “near” any given test observation.
- A fraction of available observations near of the test observation decreases exponentially by number of predictors.

(e) Now suppose that we wish to make a prediction for a test obser- vation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10 % of the train- ing observations. For p = 1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.
$$length = {0.1^{1/p}}$$
We get length 2 for p =1, we get length of $0.1^{1/2}$ for p = 2,  and for p = 100, we get length of $0.1^{1/100}$. Therefore, the length of the cube increases, while p increases.

*Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1, a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.*


4.7.10 e,f,h (for comparing "these methods" in part h, compare LDA, QDA, KNN with K=1 

This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.
```{r}
require(ISLR)
require(MASS)
require(class)
# divide into training and testing
training = Weekly[Weekly$Year<2009,]
testing = Weekly[Weekly$Year>2008,]

```

(e) Repeat (d) using LDA.
```{r}
lda.fit = lda(Direction~Lag2, data= training)
lda.fit
plot(lda.fit)

```
I fitted my LDA and now see how well it works with the test data:
```{r}
lda.pred = predict(lda.fit, newdata=testing, type="response")
lda.class = lda.pred$class
table(lda.class, testing$Direction)
```
The classifier still shows that most samples go up. But both groups don't really differ in Lag2 levels, so it's hard to set a proper boundary that differntiate them.
The error rate stays the same as it was with the Logistic Regression, 37,5%


(f) Repeat (d) using QDA.
```{r}
qda.fit = qda(Direction~Lag2, data= training)
qda.fit
```
I fitted my QDA and now see how well it works with the test data:
```{r}
qda.pred = predict(qda.fit, newdata=testing,type="response")
qda.class = qda.pred$class
table(qda.class, testing$Direction)
```
The error rate for the QDA is 41,35%, it is the worst out of all models.
It classifies aLL data, as it goes up. 

(g) Repeat (d) using KNN with K = 1.
```{r}
# set a seed for reproducibility reasons, since KNN randomly breaks ties
set.seed(1)
```
Now use the training data to predict
```{r}
train.X = cbind(training$Lag2)
test.X = cbind(testing$Lag2)
train.Y = cbind(training$Direction)
knn.pred = knn(train.X, test.X, train.Y, k=1)
table(knn.pred, testing$Direction)
```
The error rate is 50% which is bad. However, it is better than the other models to identify True Negatives, since it identifies roughly a 49% of them properly.

(h) Which of these methods appears to provide the best results on
this data?
Comparing the overall test error rates, logistic regression and LDA have the minimum error rates, followed by QDA and KNN with k = 1.





5.4.5 (for part b use the random seed 123. For part c, use seeds 234 for the 2nd split and 345 for the third split.)
In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

(a) Fit a logistic regression model that uses income and balance to predict default.
```{r}
library(ISLR)
set.seed(1)
fit.glm = glm(default ~ income + balance, data = Default, family = "binomial")
summary(fit.glm)
```
(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:
i. Split the sample set into a training set and a validation set.
```{r}
set.seed(123)
train = sample(dim(Default)[1], dim(Default)[1] / 2)
```
ii. Fit a multiple logistic regression model using only the training observations.
```{r}
fit.glm = glm(default ~ income + balance, data = Default[train,], family = "binomial")
fit.glm = glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
#Both above formulas have the same outcome.
summary(fit.glm)
```

iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.
```{r}
glm.probs = predict(fit.glm, newdata = Default[-train, ], type="response")
glm.pred=rep("No",5000)
glm.pred[glm.probs>0.5] = "Yes"
```

iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.
```{r}
mean(glm.pred != Default[-train, ]$default)
```

(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.
```{r}
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)

set.seed(234)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)

set.seed(345)
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance, data = Default, family = "binomial", subset = train)
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm <- rep("No", length(probs))
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
```
The validation estimate of the test error rate can be variable, depends on precisely which observations are included in the training set and which observations are included in the validation set.


(d) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.
```{r}
train <- sample(dim(Default)[1], dim(Default)[1] / 2)
fit.glm <- glm(default ~ income + balance + student, data = Default, family = "binomial", subset = train)
pred.glm <- rep("No", length(probs))
probs <- predict(fit.glm, newdata = Default[-train, ], type = "response")
pred.glm[probs > 0.5] <- "Yes"
mean(pred.glm != Default[-train, ]$default)
```
This does not seem that adding the “student” dummy variable leads to a reduction in the validation set estimate of the test error rate.


5.4.7

In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this ap- proach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).

(a) Fit a logistic regression model that predicts Direction using Lag1 and Lag2.
```{r}
set.seed(1)
attach(Weekly)
fit.glm <- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = "binomial")
summary(fit.glm)
```

(b) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation.
```{r}
fit.glm.1 <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = "binomial")
summary(fit.glm.1)

```

(c) Use the model from (b) to predict the direction of the first obser- vation. You can do this by predicting that the first observation will go up if P(Direction="Up"|Lag1, Lag2) > 0.5. Was this ob- servation correctly classified?
```{r}
predict.glm(fit.glm.1, Weekly[1, ], type = "response") > 0.5
```
I can conclude that the first observation is “Up”. This observation was not correctly classified as the true direction is “Down”.


(d) Write a for loop from i=1 to i=n,where n is the number of observations in the data set, that performs each of the following steps:
i. Fit a logistic regression model using all but the ith obser- vation to predict Direction using Lag1 and Lag2.
ii. Compute the posterior probability of the market moving up for the ith observation.
iii. Use the posterior probability for the ith observation in order to predict whether or not the market moves up.
iv. Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.
```{r}
error <- rep(0, dim(Weekly)[1])
for (i in 1:dim(Weekly)[1]) {
    fit.glm <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ],  family = "binomial")
    pred.up <- predict.glm(fit.glm, Weekly[i, ], type = "response") > 0.5
    true.up <- Weekly[i, ]$Direction == "Up"
    if (pred.up != true.up)
        error[i] <- 1
}
error

```

(e) Take the average of the n numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results.
```{r}
mean(error)

```
The LOOCV estimate's test error rate is 44.99541%.


