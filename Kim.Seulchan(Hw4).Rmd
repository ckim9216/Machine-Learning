---
title: "Kim.Seulchan(Hw4)"
author: "Seulchan Kim"
date: '2019 10 21 '
output: html_document
---

```{r}
library(ggplot2) 
library(xtable) 
library(magrittr) 
library(dplyr) 
library(boot) 
library(ISLR)
```

1) Use ggplot2 to create a graphic, based on the LArealestate.csv data, that shows us 3 (or more) variables on the same plot. What questions about the data set does the graphic answer?

```{r}
realestate <- read.csv("LArealestate.csv", header = TRUE)
ggplot(data = realestate, mapping = aes(x=sqft, y = price, color = beds)) + geom_line()
```

Possible questions: Does size statiscially significant influence price of LA real estate?
Does more beds statiscially significant influence price of LA real estate?
Both questions are Yes. Size and number of beds have positive linear relationship with price. 




5.4.8 We will now perform cross-validation on a simulated data set.
(a) Generate a simulated data set as follows: In this data set, what is n and what is p? Write out the model used to generate the data in equation form.

```{r}
set.seed(1)
x = rnorm(100)
y= x-2*x^2+rnorm(100)
```

- For this data, n = 100 and p = 2. The X matrix that we’re estimating is 100 × 2 because we’re estimating coefficients for X and X2. The (true) model used to generate the data then looks like:
\[Y = X-2X^2+\epsilon\]


(b) Create a scatterplot of X against Y . Comment on what you find.
```{r}
plot(y ~ x)
```
x is clearly quadratic in terms of y, obviously shows a curve relationship.


(c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:
\[Y = \beta_0 + \beta_1X+ \epsilon\]
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+\epsilon\]
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+ \beta_3X^3 + \epsilon\]
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+ \beta_3X^3 + \beta_4X^4 \epsilon\]

Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y.

\[Y = \beta_0 + \beta_1X+ \epsilon\]
```{r}
Data <- data.frame(x, y)
fit1 <- glm(y ~ x)
cv.glm(Data, fit1)$delta[1]
```
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+\epsilon\]
```{r}
Data <- data.frame(x, y)
fit2 <- glm(y ~ poly(x, degree = 2))
cv.glm(Data, fit2)$delta[1]
```
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+ \beta_3X^3 + \epsilon\]
```{r}
Data <- data.frame(x, y)
fit3 <- glm(y ~ poly(x, degree = 3))
cv.glm(Data, fit3)$delta[1]
```
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+ \beta_3X^3 + \beta_4X^4 \epsilon\]
```{r}
Data <- data.frame(x, y)
fit4 <- glm(y ~ poly(x, degree = 4))
cv.glm(Data, fit4)$delta[1]
```

(d) Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?

```{r}
set.seed(123)
Data <- data.frame(x, y)
fit1 <- glm(y ~ x)
cv.glm(Data, fit1)$delta[1]

Data <- data.frame(x, y)
fit2 <- glm(y ~ poly(x, degree = 2))
cv.glm(Data, fit2)$delta[1]

Data <- data.frame(x, y)
fit3 <- glm(y ~ poly(x, degree = 3))
cv.glm(Data, fit3)$delta[1]

Data <- data.frame(x, y)
fit4 <- glm(y ~ poly(x, degree = 4))
cv.glm(Data, fit4)$delta[1]
```
Both results are same because LOOCV evaluates n folds of a single observation. So LOOCV does not have a randomness, and it is same for any iteration under the same data and model. In other words, we are leaving out each observation exactly once and then building the same models.

(e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.
Model with polynomial degree 2 (quadratic), this is not surprising since the true model(scatterplot) is quadratic and looking at the plot values, the plotted points look obviously quadratic.

(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?
```{r}
summary(fit1)
summary(fit2)
summary(fit3)
summary(fit4)
```
We can see from the p-values of the coefficients that the 1st and 2nd degree polynomial are both significant at the 5% significance level. This makes sense because our true model includes these terms. What’s interesting is that the intercept term was also found to be significant even though the true model does not include an intercept term. Also, the 3rd and 4th degree are not significantly important. These results do agree with our results from cross-validation, since our best model is quadratic.


6.8.8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.
(a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ε of length n = 100.
```{r}
set.seed(123)
x = rnorm(100)
epsilon = rnorm(100)
```

(b) Generate a response vector Y of length n = 100 according to the model
\[Y = \beta_0 + \beta_1X+ \beta_2X^2+ \beta_3X^3+ \epsilon\], where β0, β1, β2, and β3 are constants of your choice.

```{r}
beta = sample(1:100, 4, replace=TRUE)
y= beta[1]+beta[2]*x+beta[3]*x^2+beta[4]*x^3+epsilon
# to look it up
df = data.frame(x = x, y = y)
ggplot(df, aes(x, y)) + geom_point() + theme_bw()
```

(c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2,...,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .
```{r}
library(leaps)
best.full=regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
data=data.frame(x=x,y=y),nvmax=10)
best.summary=summary(best.full)

par(mfrow=c(1,3))
plot(1:10, best.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l")
cp.min=min(best.summary$cp)

points(c(1:10)[best.summary$cp==cp.min], cp.min, pch=2, col="red")
plot(1:10, best.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l")
bic.min=min(best.summary$bic)

points(c(1:10)[best.summary$bic==bic.min], bic.min, pch=2, col="red")
plot(1:10, best.summary$adjr2,xlab="Number of Predictors", ylab="Adjusted R Square",
type="l")
adjr2.max=max(best.summary$adjr2)
points(c(1:10)[best.summary$adjr2==adjr2.max], adjr2.max, pch=2, col="red")

```
The best model selected by Cp has four predictors: X, X^2, X^3 and X^6.
The best model selected by BIC has three predictors: X, X^2 and X^3. 
The best model selected by adjusted R^2 has four predictors: X, X^2, X^3 and X^6.





(d) Repeat (c), using forward stepwise selection and also using back- wards stepwise selection. How does your answer compare to the results in (c)?

```{r}
##### Stepwise Forward Selection #####
best.frd=regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
data=data.frame(x=x,y=y),nvmax=10, method="forward")
frd.summary=summary(best.frd)
par(mfrow=c(1,3))
plot(1:10, frd.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l")
cp.min=min(frd.summary$cp)
points(c(1:10)[frd.summary$cp==cp.min], cp.min, pch=2, col="red")
plot(1:10, frd.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l")
bic.min=min(frd.summary$bic)
points(c(1:10)[frd.summary$bic==bic.min], bic.min, pch=2, col="red")
plot(1:10, frd.summary$adjr2,xlab="Number of Predictors", ylab="Adjusted R Square", type="l")
3
adjr2.max=max(frd.summary$adjr2)
points(c(1:10)[frd.summary$adjr2==adjr2.max], adjr2.max, pch=2, col="red")

### Stepwise Backward Selection ###
best.bkd=regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
data=data.frame(x=x,y=y),nvmax=10, method="backward")
bkd.summary=summary(best.bkd)
par(mfrow=c(1,3))
plot(1:10, bkd.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l")
cp.min=min(bkd.summary$cp)
points(c(1:10)[bkd.summary$cp==cp.min], cp.min, pch=2, col="red")
plot(1:10, bkd.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l")
bic.min=min(bkd.summary$bic)
points(c(1:10)[bkd.summary$bic==bic.min], bic.min, pch=2, col="red")
plot(1:10, bkd.summary$adjr2,xlab="Number of Predictors", ylab="Adjusted R Square", type="l")
adjr2.max=max(bkd.summary$adjr2)
points(c(1:10)[bkd.summary$adjr2==adjr2.max], adjr2.max, pch=2, col="red")
```

I got the same answer like (c).






